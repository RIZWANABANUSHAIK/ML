{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  IRIS Flower classification\n",
    "The data set from the kaggle:https://www.kaggle.com/datasets/arshid/iris-flower-dataset\n",
    "### imports\n",
    "-here we import torch for gradients and networks<br>\n",
    "-pandas for data processing<br>\n",
    "-sci-kit learn for its usefull functions<br>\n",
    "-shuffle for to shuffle the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler as ss\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading csv file\n",
    "* next we will drop the features not relevant to the output, and the label itself\n",
    "* storing the unique values as the classes\n",
    "* printing the distribution of the classes\n",
    "* converting the categorail fields into 1 or 0's \n",
    "* finally popping out the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./iris.csv\")\n",
    "data = data.drop([\"Id\"], axis=1)\n",
    "\n",
    "un_labels = data[\"Species\"].unique()\n",
    "# for i in data[\"Species\"].unique():\n",
    "#     print(sum(data[\"Species\"]==i))\n",
    "\n",
    "data = shuffle(data)\n",
    "labels = data.pop(\"Species\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "data = ss().fit_transform(data)\n",
    "print(un_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* next we are creating a zeros array of shape of length of labels and unique labels those are one hot labels\n",
    "* next where the labels is equal to the unique labels ,there we are equaling to 1\n",
    "* printing first 10 one hot labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_labels = np.zeros((len(labels), len(un_labels)))\n",
    "\n",
    "for i in range(len(un_labels)):\n",
    "    x = np.where(labels == un_labels[i])\n",
    "    ohe_labels[x,i] = 1\n",
    "\n",
    "ohe_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conertions\n",
    "*  converting the arrays into tensors so we can perform ml operations on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(np.array(data), dtype=torch.float32)\n",
    "labels = torch.tensor(ohe_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* creating a reasonally layered model with forst input size as 4 and ending output value as 3 followed by a softmax to convert the value predictions into probablity distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1999, 0.4581, 0.3420],\n",
      "        [0.1908, 0.4718, 0.3374],\n",
      "        [0.1999, 0.4581, 0.3420],\n",
      "        [0.2101, 0.4574, 0.3325],\n",
      "        [0.1930, 0.4684, 0.3385]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4,4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4,3),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "preds = model(features)\n",
    "print(preds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  the loss we will use is Binary cross entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6696, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "loss = loss_fn(preds, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### the optimizer we will use is SGD-(stochastic gradient descent) to update network weights during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here Saving the weights and biases of our model so we can load em up again whenever we want to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"savedweights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this is the training loop\n",
    "* we simply make predictions and calculate loss\n",
    "* then we will find gradients and optimize the model to reduce loss\n",
    "    \n",
    "* next  we are calculating the accuracy of our model\n",
    "    \n",
    "* Doing further action only on the 1/tengths of the total epochs to save time\n",
    "* here we will chech if at the index of the max pred is there a 1 present in the labels<br>  -finally calculating and printing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.28520962595939636\n",
      "Accuracy : 83.33%\n",
      "Loss : 0.19271308183670044\n",
      "Accuracy : 91.33%\n",
      "Loss : 0.14275170862674713\n",
      "Accuracy : 95.33%\n",
      "Loss : 0.10984338819980621\n",
      "Accuracy : 96.67%\n",
      "Loss : 0.08840013295412064\n",
      "Accuracy : 97.33%\n",
      "Loss : 0.07460162788629532\n",
      "Accuracy : 97.33%\n",
      "Loss : 0.06548945605754852\n",
      "Accuracy : 97.33%\n",
      "Loss : 0.059073012322187424\n",
      "Accuracy : 97.33%\n",
      "Loss : 0.05438174679875374\n",
      "Accuracy : 97.33%\n",
      "Loss : 0.050838399678468704\n",
      "Accuracy : 97.33%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100000\n",
    "for epoch in range(epochs):\n",
    "    preds = model(features)\n",
    "\n",
    "    loss = loss_fn(preds, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        if (epoch+1)%(epochs//10) == 0:\n",
    "            right = 0\n",
    "            preds=model(features)\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                ele_index=torch.where(preds[i]==preds[i].max())\n",
    "                if ohe_labels[i][ele_index]==1:\n",
    "                    right+=1   \n",
    "\n",
    "            \n",
    "            print(f\"Loss : {loss}\")\n",
    "            print(f\"Accuracy : {round(right * 100/ len(preds), 2)}%\")       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07521bee647cb038a9765ea0ecd3eab4d12b5f5b9eec9493f2d4acdd5b019b2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
